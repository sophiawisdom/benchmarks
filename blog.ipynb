{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\n",
      "The input conditions for extension module bench_shared have changed. Bumping to version 5 and re-building as bench_shared_v5...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py310_cu118/bench_shared/build.ninja...\n",
      "Building extension module bench_shared_v5...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/3] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=bench_shared_v5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /usr/local/lib/python3.10/dist-packages/torch/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.10/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_89,code=compute_89 -gencode=arch=compute_89,code=sm_89 --compiler-options '-fPIC' --keep --keep-dir /workspace/benchmark/temp --extended-lambda -std=c++17 -c /workspace/benchmark/cpp/bench_shared.cu -o bench_shared.cuda.o \n",
      "/workspace/benchmark/cpp/bench_shared.cu(226): warning #550-D: variable \"rand_val\" was set but never used\n",
      "\n",
      "/workspace/benchmark/cpp/bench_shared.cu(198): warning #177-D: variable \"our_index\" was declared but never referenced\n",
      "\n",
      "[2/3] c++ -MMD -MF bench_shared_entry.o.d -DTORCH_EXTENSION_NAME=bench_shared_v5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /usr/local/lib/python3.10/dist-packages/torch/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.10/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -c /workspace/benchmark/cpp/bench_shared_entry.cpp -o bench_shared_entry.o \n",
      "[3/3] c++ bench_shared_entry.o bench_shared.cuda.o -shared -L/usr/local/lib/python3.10/dist-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o bench_shared_v5.so\n",
      "RUNNING NOTHING for op=0 dtype=0\n",
      "RUNNING NOTHING for op=0 dtype=1\n",
      "RUNNING NOTHING for op=6 dtype=0\n",
      "RUNNING NOTHING for op=6 dtype=1\n",
      "RUNNING NOTHING for op=7 dtype=0\n",
      "RUNNING NOTHING for op=7 dtype=1\n",
      "RUNNING NOTHING for op=10 dtype=0\n",
      "RUNNING NOTHING for op=10 dtype=1\n",
      "RUNNING NOTHING for op=0 dtype=0\n",
      "RUNNING NOTHING for op=0 dtype=1\n",
      "RUNNING NOTHING for op=6 dtype=0\n",
      "RUNNING NOTHING for op=6 dtype=1\n",
      "RUNNING NOTHING for op=7 dtype=0\n",
      "RUNNING NOTHING for op=7 dtype=1\n",
      "RUNNING NOTHING for op=10 dtype=0\n",
      "RUNNING NOTHING for op=10 dtype=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading extension module bench_shared_v5...\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "128",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/workspace/benchmark/blog.ipynb Cell 1\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Brunpod/workspace/benchmark/blog.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=60'>61</a>\u001b[0m                 \u001b[39m# print(f\"{results=}\")\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Brunpod/workspace/benchmark/blog.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=62'>63</a>\u001b[0m results \u001b[39m=\u001b[39m {key: \u001b[39mdict\u001b[39m(results[key]) \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m results}\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Brunpod/workspace/benchmark/blog.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=64'>65</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mresults[our_n_threads]\u001b[39m.\u001b[39mkeys()\u001b[39m=}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Brunpod/workspace/benchmark/blog.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=66'>67</a>\u001b[0m \u001b[39m# third figure\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Brunpod/workspace/benchmark/blog.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=67'>68</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmath\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: 128"
     ]
    }
   ],
   "source": [
    "from torch.utils.cpp_extension import load\n",
    "import torch\n",
    "import random\n",
    "import statistics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "bench_shared = load(name=\"bench_shared\", sources=[\"cpp/bench_shared_entry.cpp\", \"cpp/bench_shared.cu\"], extra_cuda_cflags=[\"--keep\", \"--keep-dir\", \"/workspace/benchmark/temp\", \"--extended-lambda\"], verbose=True)\n",
    "\n",
    "outs = torch.zeros((1024*32), dtype=torch.int32, device=\"cuda\")\n",
    "\n",
    "results = defaultdict(lambda:defaultdict(list))\n",
    "\n",
    "dtypes = [\"float32\", \"int32\"]#, \"uint32\", \"half2\", \"double\", \"int64\"]\n",
    "ops = [\"add\", \"inc\", \"max\", \"xor\", \"or\", \"exch\", \"mul\", \"add_manual\", \"max_manual\", \"donothing_manual\", \"add_nochange\", \"add_warpcoalesced\"]\n",
    "\n",
    "strat = 0 # TIDX\n",
    "\n",
    "blocks = torch.cuda.get_device_properties(0).multi_processor_count\n",
    "\n",
    "our_n_threads = 128\n",
    "\n",
    "for strat in (0, 1):\n",
    "    for op, op_name in enumerate(ops):\n",
    "        if op_name not in (\"mul\", \"add\", \"add_manual\", \"add_nochange\"): continue\n",
    "        for dtype_enum, dtype in enumerate(dtypes):\n",
    "            if bench_shared.bench_shared(dtype_enum, outs, op, 128, 128, 0) == 1:\n",
    "                continue\n",
    "            # if dtype not in (\"int32\", \"inc\", \"float32\"): continue\n",
    "            for n_threads in (32, 64, 128, 256, 512, 1024):\n",
    "                if n_threads not in (our_n_threads,): continue\n",
    "                for shmem_size in [1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024]:\n",
    "                    if shmem_size > n_threads: continue\n",
    "                    clocks = []\n",
    "                    for i in range(100):\n",
    "                        outs.zero_()\n",
    "                        result = bench_shared.bench_shared(dtype_enum, outs, op, shmem_size, n_threads, strat)\n",
    "                        if result == 1:\n",
    "                            print(f\"No kernel for \")\n",
    "                            break\n",
    "\n",
    "                        outs_list = [int(a) for a in outs[:(n_threads//32)*blocks].tolist()]\n",
    "                        if outs_list.count(-1) != 0:\n",
    "                            print(f\"FOUND {outs_list.count(-1)} -1 VALUES!!!\")\n",
    "                            print(outs_list)\n",
    "                            raise AssertionError\n",
    "                        elif outs_list.count(0) != 0:\n",
    "                            print(f\"FOUND {outs_list.count(0)} 0 VALUES\")\n",
    "                            print(outs_list)\n",
    "                            raise AssertionError\n",
    "\n",
    "                        clocks.extend(outs_list)\n",
    "                    if not clocks: continue\n",
    "                    mean = int(statistics.mean(clocks))\n",
    "                    print(f\"FOR {dtype=}\\top={op_name}\\titerations=512\\t{shmem_size=}\\t{n_threads=}\\tmean: {mean}\")\n",
    "                    results[n_threads][f\"{op_name}_{dtype}_{strat}\"].append(mean)\n",
    "                    if mean < 1000:\n",
    "                        pass # print(\"\\n\\nTHIS OP AND DTYPE ARE PROBABLY EMPTY\\n\\n\")\n",
    "\n",
    "                # print(f\"{results=}\")\n",
    "\n",
    "results = {key: dict(results[key]) for key in results}\n",
    "\n",
    "print(f\"{results[our_n_threads].keys()=}\")\n",
    "\n",
    "# third figure\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(14, 10))  # Set a larger figure size for better legibility\n",
    "\n",
    "plt.plot([a / 512 for a in results[our_n_threads]['add_float32_1']], label=f\"float32 add\", color=\"blue\")\n",
    "plt.plot([a / 512 for a in results[our_n_threads]['mul_float32_1']], label=f\"float32 mul\", color=\"orange\")\n",
    "plt.plot([a / 512 for a in results[our_n_threads]['add_manual_float32_1']], label=f\"float32 add manual\", color=\"green\")\n",
    "plt.plot([a / 512 for a in results[our_n_threads]['add_nochange_float32_1']], label=f\"float32 add nochange\", color=\"yellow\")\n",
    "plt.plot([a / 512 for a in results[our_n_threads]['add_int32_1']], label=f\"int32 add\", color=\"red\")\n",
    "\n",
    "num_ticks = int(math.log(our_n_threads, 2)+1)\n",
    "\n",
    "device = \"RTX 4090\" if \"4090\" in torch.cuda.get_device_name() else torch.cuda.get_device_name()\n",
    "plt.title(f\"Shared memory atomic add throughput on {device} (n_threads={our_n_threads})\", fontsize=16)\n",
    "plt.ylabel(\"Clock cycles per iteration\", fontsize=14)\n",
    "plt.xlabel(\"Contending threads\", fontsize=14)\n",
    "plt.yscale(\"log\")\n",
    "plt.xticks(list(range(num_ticks)), [math.ceil(our_n_threads / (2**i)) for i in range(num_ticks)], fontsize=12)\n",
    "plt.yticks([10, 100, 1000, 10000], [\"10\", \"100\", \"1,000\", \"10,000\"], fontsize=12)\n",
    "plt.grid(True, which=\"both\", linestyle='--', linewidth=0.5)\n",
    "\n",
    "# Adjust x-axis limits to remove whitespace\n",
    "plt.xlim(0, num_ticks - 1)\n",
    "\n",
    "plt.legend(loc='upper right', bbox_to_anchor=(1, 1), fontsize='large', ncol=1)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 0.85, 1])  # Adjust layout to make space for the legends\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results[our_n_threads]['add_float32_1']=[5741861, 2640494, 1298197, 752369, 466275, 325790, 295642, 276991]\n",
      "results[our_n_threads]['add_nochange_float32_1']=[5741861, 2625372, 1293768, 750373, 463591, 326207, 295566, 276757]\n"
     ]
    }
   ],
   "source": [
    "print(f\"{results[our_n_threads]['add_float32_1']=}\")\n",
    "print(f\"{results[our_n_threads]['add_nochange_float32_1']=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first figure\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(14, 10))  # Set a larger figure size for better legibility\n",
    "\n",
    "plt.plot([a / 512 for a in results[our_n_threads]['add_float32']], label=f\"float32\", color=\"blue\")\n",
    "plt.plot([a / 512 for a in results[our_n_threads]['add_int32']], label=f\"int32\", color=\"red\")\n",
    "\n",
    "device = \"RTX 4090\" if \"4090\" in torch.cuda.get_device_name() else torch.cuda.get_device_name()\n",
    "plt.title(f\"Shared memory atomic add throughput on {device} (n_threads=256)\", fontsize=16)\n",
    "plt.ylabel(\"Clock cycles per iteration\", fontsize=14)\n",
    "plt.xlabel(\"Contending threads\", fontsize=14)\n",
    "plt.yscale(\"log\")\n",
    "plt.xticks(list(range(num_ticks)), [math.ceil(256 / (2**i)) for i in range(num_ticks)], fontsize=12)\n",
    "plt.yticks([10, 100, 1000, 10000], [\"10\", \"100\", \"1,000\", \"10,000\"], fontsize=12)\n",
    "plt.grid(True, which=\"both\", linestyle='--', linewidth=0.5)\n",
    "\n",
    "# Adjust x-axis limits to remove whitespace\n",
    "plt.xlim(0, num_ticks - 1)\n",
    "\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5), fontsize='small', ncol=1)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 0.85, 1])  # Adjust layout to make space for the legends\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# second figure\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(14, 10))  # Set a larger figure size for better legibility\n",
    "\n",
    "# plt.plot([a / 1024 for a in results[256]['max_int32']], label=f\"max int32\", color=\"green\")\n",
    "plt.plot([a / 512 for a in results[our_n_threads]['max_float32']], label=f\"max float32 from int32\", color=\"blue\")\n",
    "plt.plot([a / 512 for a in results[our_n_threads]['max_manual_float32']], label=f\"max float32 with CAS\", color=\"red\")\n",
    "plt.plot([a / 512 for a in results[our_n_threads]['donothing_manual']], label=f\"CAS do nothing\", color=\"green\")\n",
    "\n",
    "device = \"RTX 4090\" if \"4090\" in torch.cuda.get_device_name() else torch.cuda.get_device_name()\n",
    "plt.title(f\"Shared memory atomic max throughput on {device} (n_threads=256)\", fontsize=16)\n",
    "plt.ylabel(\"Clock cycles per iteration\", fontsize=14)\n",
    "plt.xlabel(\"Contending threads\", fontsize=14)\n",
    "plt.yscale(\"log\")\n",
    "plt.xticks(list(range(num_ticks)), [math.ceil(our_n_threads / (2**i)) for i in range(num_ticks)], fontsize=12)\n",
    "plt.yticks([10, 100, 1000, 10000], [\"10\", \"100\", \"1,000\", \"10,000\"], fontsize=12)\n",
    "plt.grid(True, which=\"both\", linestyle='--', linewidth=0.5)\n",
    "\n",
    "# Adjust x-axis limits to remove whitespace\n",
    "plt.xlim(0, 9 - 1)\n",
    "\n",
    "plt.legend(loc='upper right', bbox_to_anchor=(1, 1), fontsize='large', ncol=1)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 0.85, 1])  # Adjust layout to make space for the legends\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# third figure\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for strat in (0, 1):\n",
    "    plt.figure(figsize=(14, 10))  # Set a larger figure size for better legibility\n",
    "\n",
    "    plt.plot([a / 512 for a in results[our_n_threads][f'add_float32_{strat}']], label=f\"float32 add\", color=\"blue\")\n",
    "    plt.plot([a / 512 for a in results[our_n_threads][f'mul_float32_{strat}']], label=f\"float32 mul\", color=\"orange\")\n",
    "    plt.plot([a / 512 for a in results[our_n_threads][f'add_manual_float32_{strat}']], label=f\"float32 add manual\", color=\"green\")\n",
    "    plt.plot([a / 512 for a in results[our_n_threads][f'add_nochange_float32_{strat}']], label=f\"float32 add nochange\", color=\"yellow\")\n",
    "    plt.plot([a / 512 for a in results[our_n_threads][f'add_int32_{strat}']], label=f\"int32 add\", color=\"red\")\n",
    "\n",
    "\n",
    "    device = \"RTX 4090\" if \"4090\" in torch.cuda.get_device_name() else torch.cuda.get_device_name()\n",
    "    plt.title(f\"Shared memory atomic add throughput on {device} (n_threads=128) {strat=}\", fontsize=16)\n",
    "    plt.ylabel(\"Clock cycles per iteration\", fontsize=14)\n",
    "    plt.xlabel(\"Contending threads\", fontsize=14)\n",
    "    plt.yscale(\"log\")\n",
    "    plt.xticks(list(range(num_ticks)), [math.ceil(our_n_threads / (2**i)) for i in range(num_ticks)], fontsize=12)\n",
    "    plt.yticks([10, 100, 1000, 10000], [\"10\", \"100\", \"1,000\", \"10,000\"], fontsize=12)\n",
    "    plt.grid(True, which=\"both\", linestyle='--', linewidth=0.5)\n",
    "\n",
    "    # Adjust x-axis limits to remove whitespace\n",
    "    plt.xlim(0, num_ticks - 1)\n",
    "\n",
    "    plt.legend(loc='upper right', bbox_to_anchor=(1, 1), fontsize='large', ncol=1)\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 0.85, 1])  # Adjust layout to make space for the legends\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[256]['max_manual_float32']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[256]['donothing_manual_int32']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[256].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
